# ğŸ” RAG Pipeline (Retrieval-Augmented Generation)

A modular Python-based implementation of a simple Retrieval-Augmented Generation (RAG) system using keyword-based retrieval and LiteLLM for response generation.

This project demonstrates how modern AI systems combine **retrieval + generation** to produce grounded and context-aware responses.

---

## ğŸ¯ Objective

Design and implement a simple RAG workflow:

```
User Question â†’ Retrieve Relevant Documents â†’ Send Context to LLM â†’ Generate Answer
```

The goal is to understand how LLMs can reduce hallucination by grounding outputs in external knowledge sources.

---

## ğŸš€ Features

* ğŸ“š Local knowledge base using text files
* ğŸ” Keyword-based document retrieval
* ğŸ¤– LLM integration via LiteLLM
* ğŸ” Secure API key handling with dotenv
* ğŸ’» Interactive CLI interface
* ğŸ§© Clean modular architecture

---

## ğŸ› ï¸ Tech Stack

* Python
* LiteLLM
* Groq API
* python-dotenv

---

## ğŸ“¦ Installation

### 1. Clone the repository

```bash
git clone https://github.com/PratheekshaSNaik/RAG_Pipeline.git
cd RAG_Pipeline
```

### 2. Install dependencies

Since no virtual environment is used, install globally:

```bash
python -m pip install -r requirements.txt
```

Or manually:

```bash
python -m pip install litellm python-dotenv
```

---

## ğŸ” Setup Environment Variables

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_key_here
```

âš ï¸ Make sure your `.env` file is NOT pushed to GitHub.

---

## ğŸš« Important (Security)

Ensure `.env` is added to `.gitignore`:

```
.env
```

---

## â–¶ï¸ Run the Project

```bash
python main.py
```

The application will:

1. Accept a question via CLI
2. Retrieve relevant documents from `knowledge_base/`
3. Send retrieved context to the LLM
4. Generate and display the final answer

---

## ğŸ— Architecture Overview

### 1ï¸âƒ£ Retriever Layer (`retriever.py`)

* Loads documents from `knowledge_base/`
* Performs simple keyword-based matching
* Returns the most relevant document content

### 2ï¸âƒ£ LLM Layer (`llm_layer.py`)

* Connects to Groq via LiteLLM
* Sends retrieved context + user query
* Generates final response

### 3ï¸âƒ£ RAG Pipeline (`rag_pipeline.py`)

* Orchestrates retrieval and generation phases
* Acts as the workflow controller

### 4ï¸âƒ£ CLI Interface (`main.py`)

* Handles user interaction
* Passes queries into the RAG pipeline

---

## ğŸ“ Project Structure

```
RAG_PIPELINE/
â”‚
â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ llm_layer.cpython-314.pyc
â”‚   â”œâ”€â”€ rag_pipeline.cpython-314.pyc
â”‚   â””â”€â”€ retriever.cpython-314.pyc
â”‚
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ ai_notes.txt
â”‚   â””â”€â”€ ml_notes.txt
â”‚
â”œâ”€â”€ llm_layer.py
â”œâ”€â”€ retriever.py
â”œâ”€â”€ rag_pipeline.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“ Learning Outcomes

After completing this project, you will:

* Understand the fundamentals of RAG architecture
* Learn how retrieval improves LLM response grounding
* Build modular AI pipelines
* Separate retrieval logic from generation logic
* Implement secure API handling

---

## ğŸš€ Why This Matters

Modern AI systems are not standalone LLM calls.

They use:

* Retrieval systems
* Knowledge bases
* Modular orchestration
* Secure configuration management

This project builds foundational understanding of how production-level RAG systems are structured.

---

## ğŸ”® Future Improvements

* Replace keyword retrieval with embedding-based semantic search
* Integrate FAISS or a vector database
* Implement document chunking & scoring
* Add Streamlit / FastAPI web interface
* Add evaluation metrics

---

## ğŸ™Œ Acknowledgements

Groq for high-speed LLM inference

LiteLLM for unified model access

Open-source community
